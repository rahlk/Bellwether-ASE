@inproceedings{posnet11,
abstract = {Software systems are decomposed hierarchically, for example, into modules, packages and files. This hierarchical decomposition has a profound influence on evolvability, maintainability and work assignment. Hierarchical decomposition is thus clearly of central concern for empirical software engineering researchers; but it also poses a quandary. At what level do we study phenomena, such as quality, distribution, collaboration and productivity? At the level of files? packages? or modules? How does the level of study affect the truth, meaning, and relevance of the findings? In other fields it has been found that choosing the wrong level might lead to misleading or fallacious results. Choosing a proper level, for study, is thus vitally important for empirical software engineering research; but this issue hasn't thus far been explicitly investigated. We describe the related idea of ecological inference and ecological fallacy from sociology and epidemiology, and explore its relevance to empirical software engineering; we also present some case studies, using defect and process data from 18 open source projects to illustrate the risks of modeling at an aggregation level in the context of defect prediction, as well as in hypothesis testing.},
author = {Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar},
booktitle = {2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)},
doi = {10.1109/ASE.2011.6100074},
file = {:Users/rkrsn/Documents/Mendeley Desktop//Posnett, Filkov, Devanbu - Ecological inference in empirical software engineering.pdf:pdf},
isbn = {978-1-4577-1639-3},
issn = {1938-4300},
mendeley-groups = {ASE 2016},
month = {nov},
pages = {362--371},
publisher = {IEEE},
title = {{Ecological inference in empirical software engineering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6100074},
year = {2011}
}
@inproceedings{yang11,
abstract = {Background: Continuously calibrated and validated parametric models are necessary for realistic software estimates. However, in practice, variations in model adoption and usage patterns introduce a great deal of local bias in the resultant historical data. Such local bias should be carefully examined and addressed before the historical data can be used for calibrating new versions of parametric models. Aims: In this study, we aim at investigating the degree of such local bias in a cross-company historical dataset, and assessing its impacts on parametric estimation model's performance. Method: Our study consists of three parts: 1) defining a method for measuring and analyzing the local bias associated with individual organization data subset in the overall dataset; 2) assessing the impacts of local bias on the estimation performance of COCOMO II 2000 model; 3) performing a correlation analysis to verify that local bias can be harmful to the performance of a parametric estimation model. Results: Our results show that the local bias negatively impacts the performance of parametric model. Our measure of local bias has a positive correlation with the performance by statistical importance. Conclusion: Local calibration by using the whole multi-company data would get worse performance. The influence of multi-company data could be defined by local bias and be measured by our method.},
address = {New York, New York, USA},
author = {Yang, Ye and Xie, Lang and He, Zhimin and Li, Qi and Nguyen, Vu and Boehm, Barry and Valerdi, Ricardo},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering - Promise '11},
doi = {10.1145/2020390.2020404},
file = {:Users/rkrsn/Documents/Mendeley Desktop//Yang et al. - Local bias and its impacts on the performance of parametric estimation models.pdf:pdf},
isbn = {9781450307093},
keywords = {accuracy indicator,all or part of,effort estimation,local bias,or hard copies of,parametric model,permission to make digital,this work for},
mendeley-groups = {ASE 2016},
pages = {1--10},
publisher = {ACM Press},
title = {{Local bias and its impacts on the performance of parametric estimation models}},
url = {http://dl.acm.org/citation.cfm?doid=2020390.2020404},
year = {2011}
}
@inproceedings{me12d,
abstract = {Abstractâ€”Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules. This instability can be explained by data heterogeneity. We show that effort and defect data contain many local regions with markedly different properties to the global space. In other words, what appears to be useful in a global context is often irrelevant for particular local contexts. This result raises questions about the generality of conclusions from empirical SE. At the very least, SE researchers should test if their supposedly general conclusions are valid within subsets of their data. At the very most, empirical SE should become a search for local regions with similar properties (and conclusions should be constrained to just those regions).},
author = {Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
booktitle = {2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)},
doi = {10.1109/ASE.2011.6100072},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Menzies et al. - Local vs. global models for effort estimation and defect prediction.pdf:pdf},
isbn = {978-1-4577-1639-3},
keywords = {Data mining,I,defect/effort estimation,empirical SE.,validation},
mendeley-groups = {ASE 2015 Lit},
month = {nov},
pages = {343--351},
publisher = {IEEE},
title = {{Local vs. global models for effort estimation and defect prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6100072},
year = {2011}
}
@inproceedings{Bettenburg2012,
abstract = {Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds.},
author = {Bettenburg, Nicolas and Nagappan, Meiyappan and Hassan, Ahmed E.},
booktitle = {2012 9th IEEE Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2012.6224300},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Bettenburg, Nagappan, Hassan - Think locally, act globally Improving defect and effort prediction models(2).pdf:pdf},
isbn = {978-1-4673-1761-0},
issn = {21601852},
keywords = {models,software metrics,techniques},
mendeley-groups = {ASE 2016},
month = {jun},
pages = {60--69},
publisher = {IEEE},
title = {{Think locally, act globally: Improving defect and effort prediction models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6224300},
year = {2012}
}
@article{ma12,
abstract = {Context: Software defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction? Objective: In this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model. Method: Unlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built. Results: This article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods. Conclusion: It is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Ma, Ying and Luo, Guangchun and Zeng, Xue and Chen, Aiguo},
doi = {10.1016/j.infsof.2011.09.007},
file = {:Users/rkrsn/Documents/Mendeley Desktop//Ma et al. - Transfer learning for cross-company software defect prediction - 2012.pdf:pdf},
issn = {09505849},
journal = {Inf. Softw. Technol.},
keywords = {Different distribution,Machine learning,Naive Bayes,Software defect prediction,Transfer learning},
month = {mar},
number = {3},
pages = {248--256},
title = {{Transfer learning for cross-company software defect prediction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584911001996},
volume = {54},
year = {2012}
}
